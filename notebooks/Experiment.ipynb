{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49774185",
   "metadata": {},
   "source": [
    "## Text Summarization Using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25963682",
   "metadata": {},
   "source": [
    "---\n",
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ebd04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.25.1\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM\n",
    "from transformers import AdamWeightDecay\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaf0eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets = raw_datasets.save_to_disk(\"/home/jvdboss/workspace/Testing_Stuff/summarization_tensorflow/final_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8674c39",
   "metadata": {},
   "source": [
    "---\n",
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0f2cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_from_disk(\"/home/jvdboss/workspace/Testing_Stuff/summarization_tensorflow/final_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "143fb73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 204045\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 11332\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 11334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check out the data \n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84effa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_samples(dataset, num_examples=5):\n",
    "    assert num_examples <= len(\n",
    "        dataset\n",
    "    ), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset) - 1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset) - 1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5c1ee61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Father-of-two Darren Mark Harvey, 25, of Portsmouth Road, Woolston, died in the crash in the New Forest in August.\\nThe coroner concluded Mr Harvey died as a result of a road traffic collision and it was probable that he steered into the vehicle in a deliberate act.\\nHe said he could not be absolutely sure Mr Harvey had intended to kill himself.\\nThe crash happened on the A337 close to the Holland Wood Campsite near Brockenhurst late on Saturday 22 August last year.\\nMr Harvey's wife, Rebecca, told the inquest that on the night of his death, Mr Harvey had left home in such a distressed state, she had called the police.\\nThe inquest was shown footage from a camera mounted on the dashboard of the fire engine that showed Mr Harvey's car swerve into its path.\\nHe had a history of depression and was on anti-depressants, the inquest was told.\\nMr Harvey's family described him as a \"proud dad, a loving son, brother and grandson who recently married\".</td>\n",
       "      <td>A man who died in a head-on collision with a fire engine probably steered into the vehicle deliberately, an inquest has heard.</td>\n",
       "      <td>35314839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A US Senate investigation said the UK-based bank had been a conduit for \"drug kingpins and rogue nations\".\\nMoney laundering is the process of disguising the proceeds of crime so that the money cannot be linked to the wrongdoing.\\nHSBC admitted having poor money laundering controls and apologised.\\n\"We accept responsibility for our past mistakes,\" said HSBC group chief executive Stuart Gulliver in a statement.\\n\"We have said we are profoundly sorry for them, and we do so again.\"\\nThe bank said it had spent $290m on improving its systems to prevent money laundering and clawed back some bonuses paid to senior executives in the past.\\nBy Robert PestonBusiness editor\\nIt also said it expected to reach an agreement with the UK's Financial Services Authority shortly.\\nLast month it announced it had set aside $1.5bn to cover the costs of any settlement or fines.\\nThe news followed the announcement of a similar but much smaller settlement with UK-based Standard Chartered bank, which will pay $300m in fines for violating US sanctions.\\nThe cases are seen as part of a crackdown on money laundering and sanctions violations being led by federal government agencies and New York state authorities.\\nThe settlement had been widely expected following a report by the US Senate, published earlier this year, that was heavily critical of HSBC's money laundering controls.\\nThe report alleged that:\\nThe report suggested HSBC accounts in Mexico and the US were being used by drug barons to launder money.\\n\"The banks became very overextended, not just in lending on property, which we all know about, but in this case, for example, buying businesses in Mexico about which, it turned out, they knew too little,\" Sir John Gieve, former deputy governor of the Bank of England told the BBC.\\nThe Senate report also said HSBC regularly circumvented restrictions on dealings with Iran, North Korea, and other states subject to US sanctions.\\nBBC business editor Robert Peston said that as big as the $1.9bn penalty looks, it could have been much worse.\\n\"HSBC has signed a Deferred Prosecution Agreement for breaches of the US Bank Secrecy Act, the Trading with the Enemy Act and assorted money laundering offences. This is in effect putting the bank on probation,\" he said.\\n\"But if HSBC had been indicted for these offences, that would have meant that the US government and others could no longer have conducted business with it, which would have been humiliating and highly damaging.\"\\nThe bank stressed that it had taken on new senior management since the time the problems happened.\\nLord Green was chairman of HSBC from 2006 until late 2010 and is now Minister of State for Trade and Investment.\\nIn a statement, his department said: \"The report by the US Senate Sub-Committee sets out in detail the evidence submitted to it, and the action taken by HSBC to ensure compliance with US regulations at the time that Lord Green was group chairman. It is for HSBC to respond to this report.\\n\"At the time of the report's publication HSBC expressed its regret that there were failures of implementation and Lord Green has said that he shares that regret.\"\\nHSBC has announced it has appointed a former US official to work as its head of financial crime compliance, which is a new position.\\nBob Werner was previously the head of the US Treasury's Office of Foreign Assets Control (OFAC) - the agency responsible for enforcing the US sanctions on countries including Iran.\\nHe will be responsible for beefing up HSBC's anti-money laundering and sanctions compliance systems.\\nIt is unclear what impact the case will have on HSBC's business. The bank is the biggest in Europe by market capitalisation, and made pre-tax profits of $12.7bn for the first six months of 2012.</td>\n",
       "      <td>HSBC has confirmed it is to pay US authorities $1.9bn (£1.2bn) in a settlement over money laundering, the largest paid in such a case.</td>\n",
       "      <td>20673466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The club allege Charles Green, Imran Ahmad, Brian Stockbridge and Derek Llambias negotiated commercial deals with Sports Direct below market value.\\nThey also believe Mr Ashley unfairly benefited from the alleged negligence.\\nThe action came to light at the Court of Session where Mr Ashley's legal team succeeded in a bid to get Rangers to disclose documents to them.\\nRangers allege that the former directors did not act in the business's best interests when they negotiated commercial deals with Sports Direct.\\nThe deals which the club are objecting to include an October 2012 agreement in which the directors allowed Sports Direct to have the naming rights to Ibrox Stadium.\\nRangers believe that the naming rights were given to Sports Direct at a price well below their true market value.\\nIn November 2014, the club also entered into a Partnership Marketing Agreement with Sports Direct which allowed the business advertising space at Ibrox.\\nThe club's current management believe that the deal was also arranged at a price well blow its true market value.\\nThey also believe that Mr Ashley unfairly benefited from the alleged negligence displayed by Mr Green and his colleagues.\\nRangers hope to recover a total of £4,106,470.83 from the action.\\nThey also want a judge to declare the Partnership Marketing Agreement void.\\nThe action is concerned with the alleged actions involving the club's former chief executive Charles Green, commercial director Imran Ahmad and its former finance director Brian Stockbridge.\\nThe club is also concerned with the alleged conduct of Mr Green's replacement as chief executive, Derek Llambias.\\nThe case will next call before the court in the near future.</td>\n",
       "      <td>Rangers have launched a £4m legal claim against a number of former directors and Sports Direct owner Mike Ashley.</td>\n",
       "      <td>37058224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Paul Wilmott, 63, died in the blast at his home in Haxby, near York, on 19 February 2016.\\nMr Wilmott may have become desensitised to the smell of gas before the blast, caused by a fracture in a corroded gas pipe, jurors were told.\\nThey returned a verdict of accidental death earlier.\\nLive updates on this story and others in North Yorkshire\\nSteve Critchlow, from the Health and Safety Executive, said the house had been filled with gas leaking from a pipe buried in Mr Wilmott's concrete floor.\\nHe said: \"I would imagine that probably an hour, maximum, would be enough to create this sort of incident.\"\\nGiven the amount of gas present, a small electrical spark from a contact switch would have been sufficient to cause the explosion, he added.\\nHe said it was not uncommon for people to become desensitised to the smell of leaking gas if they were asleep when the leak began.\\nThe house in Springwood was built in the 1970s and had not been built to modern standards that may have protected the pipe from corrosion, jurors were told.\\nForensic metallurgist Dr Elizabeth Geary told the inquest in York that formic acid produced by an ants' nest found in a wall nearby may have contributed to the pipe corrosion.\\nAn inquiry found the copper pipe fractured at a point where two concrete floor slabs met and had moved \"possibly as a result of bad weather\", coroner Rob Turnbull said.</td>\n",
       "      <td>A gas explosion that killed a man was potentially triggered when he switched on a light or his kettle, an inquest has heard.</td>\n",
       "      <td>39568100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 2016 Masters champion shot a nine-over-par opening round of 81 on Thursday, the joint second worst score of the day.\\nThe 29-year-old, who has a history of back injuries, was due to tee off at 19:58 BST at Erin Hills in Wisconsin.\\n\"Another disappointing week comes to an end,\" said Willett on social media.\\nHe added: \"Working hard to get back but the body and swing not allowing it, sorry guys.\"\\nWillett also pulled out of last month's Players Championship at Sawgrass midway through his second round and missed the World Cup in November to avoid aggravating his back problem.\\nAt Augusta in April, he became the first defending Masters champion since Mike Weir in 2004 to miss the cut and split with caddie Jonathan Smart last month.\\nHe blew a three-shot lead in the final round of the Maybank Championship in Malaysia in February to finish tied fifth.</td>\n",
       "      <td>England's Danny Willett has withdrawn from the US Open before starting his second round due to an ongoing back injury.</td>\n",
       "      <td>40309456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_samples(raw_datasets['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc83a72e",
   "metadata": {},
   "source": [
    "---\n",
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "848ad3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Type of Data\n",
    "type(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df2af820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the metric\n",
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9984de4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"rouge\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}, usage: \"\"\"\n",
       "Calculates average rouge scores for a list of hypotheses and references\n",
       "Args:\n",
       "    predictions: list of predictions to score. Each prediction\n",
       "        should be a string with tokens separated by spaces.\n",
       "    references: list of reference for each prediction. Each\n",
       "        reference should be a string with tokens separated by spaces.\n",
       "    rouge_types: A list of rouge types to calculate.\n",
       "        Valid names:\n",
       "        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n",
       "        `\"rougeL\"`: Longest common subsequence based scoring.\n",
       "        `\"rougeLSum\"`: rougeLsum splits text using `\"\n",
       "\"`.\n",
       "        See details in https://github.com/huggingface/datasets/issues/617\n",
       "    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n",
       "    use_aggregator: Return aggregates if this is set to True\n",
       "Returns:\n",
       "    rouge1: rouge_1 (precision, recall, f1),\n",
       "    rouge2: rouge_2 (precision, recall, f1),\n",
       "    rougeL: rouge_l (precision, recall, f1),\n",
       "    rougeLsum: rouge_lsum (precision, recall, f1)\n",
       "Examples:\n",
       "\n",
       "    >>> rouge = datasets.load_metric('rouge')\n",
       "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
       "    >>> references = [\"hello there\", \"general kenobi\"]\n",
       "    >>> results = rouge.compute(predictions=predictions, references=references)\n",
       "    >>> print(list(results.keys()))\n",
       "    ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
       "    >>> print(results[\"rouge1\"])\n",
       "    AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))\n",
       "    >>> print(results[\"rouge1\"].mid.fmeasure)\n",
       "    1.0\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7e80793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_preds = [\"hello there\", \"general kenobi\"]\n",
    "fake_labels = [\"hello there\", \"general kenobi\"]\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9994aad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model checkpoint of pretrained model \n",
    "model_checkpoint = \"t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecbe91ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37494fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [8774, 6, 48, 19, 3, 9, 7142, 55, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Tokenizer on different sentences for checking\n",
    "tokenizer(\"Hello, this is a sentence!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b466d449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[8774, 6, 48, 19, 3, 9, 7142, 55, 1], [100, 19, 430, 7142, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"Hello, this is a sentence!\", \"This is another sentence.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b486944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[8774, 6, 48, 19, 3, 9, 7142, 55, 1], [100, 19, 430, 7142, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    print(tokenizer([\"Hello, this is a sentence!\", \"This is another sentence.\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da140da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using one of the five T5 checkpoints we have to prefix the inputs with \"summarize:\" \n",
    "#(the model can also translate and it needs the prefix to know which task it has to perform).\n",
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"summarize: \"\n",
    "else:\n",
    "    prefix = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4873d9ab",
   "metadata": {},
   "source": [
    "We have written the function that will preprocess our samples. We just feed them to the tokenizer with the argument truncation=True. This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model. The padding will be dealt with later on (in a data collator) so we pad examples to the longest length in the batch and not the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fca117ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"summary\"], max_length=max_target_length, truncation=True\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "513ac469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[21603, 10, 37, 423, 583, 13, 1783, 16, 20126, 16496, 6, 80, 13, 8, 844, 6025, 4161, 6, 19, 341, 271, 14841, 5, 7057, 161, 19, 4912, 16, 1626, 5981, 11, 186, 7540, 16, 1276, 15, 2296, 7, 5718, 2367, 14621, 4161, 57, 4125, 387, 5, 15059, 7, 30, 8, 4653, 4939, 711, 747, 522, 17879, 788, 12, 1783, 44, 8, 15763, 6029, 1813, 9, 7472, 5, 1404, 1623, 11, 5699, 277, 130, 4161, 57, 18368, 16, 20126, 16496, 227, 8, 2473, 5895, 15, 147, 89, 22411, 139, 8, 1511, 5, 1485, 3271, 3, 21926, 9, 472, 19623, 5251, 8, 616, 12, 15614, 8, 1783, 5, 37, 13818, 10564, 15, 26, 3, 9, 3, 19513, 1481, 6, 18368, 186, 1328, 2605, 30, 7488, 1887, 3, 18, 8, 711, 2309, 9517, 89, 355, 5, 3966, 1954, 9233, 15, 6, 113, 293, 7, 8, 16548, 13363, 106, 14022, 84, 47, 14621, 4161, 6, 243, 255, 228, 59, 7828, 8, 1249, 18, 545, 11298, 1773, 728, 8, 8347, 1560, 5, 611, 6, 255, 243, 72, 1709, 1528, 161, 228, 43, 118, 4006, 91, 12, 766, 8, 3, 19513, 1481, 410, 59, 5124, 5, 96, 196, 17, 19, 1256, 68, 27, 103, 317, 132, 19, 78, 231, 23546, 21, 970, 51, 89, 2593, 11, 8, 2504, 189, 3, 18, 11, 27, 3536, 3653, 24, 3, 18, 68, 34, 19, 966, 114, 62, 31, 60, 23708, 42, 11821, 976, 255, 243, 5, 96, 11880, 164, 59, 36, 1176, 68, 34, 19, 2361, 82, 3503, 147, 8, 336, 360, 477, 5, 96, 17891, 130, 25, 59, 1065, 12, 199, 178, 3, 9, 720, 72, 116, 8, 6337, 11, 8, 6196, 5685, 7, 141, 2767, 91, 4609, 7940, 6, 3, 9, 8347, 5685, 3048, 16, 286, 640, 8, 17600, 7, 250, 13, 8, 3917, 3412, 5, 1276, 15, 2296, 7, 47, 14621, 1560, 57, 982, 6, 13233, 53, 3088, 12, 4277, 72, 13613, 7, 16, 8, 616, 5, 12580, 17600, 7, 2063, 65, 474, 3, 9, 570, 30, 165, 475, 13, 8, 7540, 6025, 4161, 11, 3863, 43, 118, 3, 19492, 59, 12, 9751, 12493, 3957, 5, 37, 16117, 3450, 31, 7, 21108, 12580, 2488, 5104, 11768, 1306, 47, 16, 1626, 5981, 30, 2089, 12, 217, 8, 1419, 166, 609, 5, 216, 243, 34, 47, 359, 12, 129, 8, 8347, 1711, 515, 269, 68, 3, 9485, 3088, 12, 1634, 95, 8, 433, 5, 96, 196, 47, 882, 1026, 3, 9, 1549, 57, 8, 866, 13, 1783, 24, 65, 118, 612, 976, 3, 88, 243, 5, 96, 14116, 34, 19, 842, 18, 18087, 21, 151, 113, 43, 118, 5241, 91, 13, 70, 2503, 11, 8, 1113, 30, 1623, 535, 216, 243, 34, 47, 359, 24, 96, 603, 5700, 342, 2245, 121, 130, 1026, 12, 1822, 8, 844, 167, 9930, 11, 3, 9, 964, 97, 3869, 474, 16, 286, 21, 8347, 9793, 1390, 5, 2114, 25, 118, 4161, 57, 18368, 16, 970, 51, 89, 2593, 11, 10987, 32, 1343, 42, 8, 17600, 7, 58, 8779, 178, 81, 39, 351, 13, 8, 1419, 11, 149, 34, 47, 10298, 5, 8601, 178, 30, 142, 40, 157, 12546, 5, 15808, 1741, 115, 115, 75, 5, 509, 5, 1598, 42, 146, 51, 89, 2593, 1741, 115, 115, 75, 5, 509, 5, 1598, 5, 1], [21603, 10, 71, 1472, 6196, 877, 326, 44, 8, 9108, 86, 29, 16, 6000, 1887, 44, 81, 11484, 10, 1755, 272, 4209, 30, 1856, 11, 2554, 130, 1380, 12, 1175, 8, 1595, 5, 282, 79, 3, 9094, 1067, 79, 1509, 8, 192, 14264, 6, 3, 16669, 596, 18, 969, 18, 1583, 16, 8, 443, 2447, 6, 3, 35, 6106, 19565, 57, 12314, 7, 5, 555, 13, 8, 1552, 1637, 19, 45, 3434, 6, 8, 119, 45, 1473, 11, 14441, 5, 94, 47, 70, 166, 706, 16, 5961, 5316, 5, 37, 2535, 13, 80, 13, 8, 14264, 243, 186, 13, 8, 9234, 141, 646, 525, 12770, 7, 30, 1476, 11, 175, 141, 118, 10932, 5, 2867, 1637, 43, 13666, 3709, 11210, 11, 56, 1731, 70, 1552, 13, 8, 3457, 4939, 865, 145, 79, 141, 4355, 5, 5076, 43, 3958, 15, 26, 21, 251, 81, 8, 3211, 5, 86, 7, 102, 1955, 24723, 243, 10, 96, 196, 17, 3475, 38, 713, 8, 1472, 708, 365, 80, 13, 8, 14264, 274, 16436, 12, 8, 511, 5, 96, 27674, 8, 2883, 1137, 19, 341, 365, 4962, 6, 34, 19, 816, 24, 8, 1472, 47, 708, 24067, 535, 1], [21603, 10, 21945, 4283, 16, 3, 9, 1102, 12, 1921, 552, 8, 804, 14941, 7, 6, 116, 8, 11808, 23038, 70, 6217, 12, 281, 985, 3, 9, 511, 964, 13, 8, 1131, 2948, 5, 19438, 20495, 1625, 56, 456, 1025, 2177, 13, 372, 18, 5058, 6777, 23, 13016, 157, 4470, 35, 5, 37, 296, 6336, 3, 14064, 3, 28423, 19372, 21, 3, 60, 2660, 53, 16, 8, 7688, 3, 8102, 6, 84, 228, 43, 894, 376, 27614, 13, 11148, 5, 299, 3, 849, 2239, 7, 163, 14014, 13450, 3, 9, 3, 60, 8234, 232, 6, 227, 3, 19585, 643, 8, 3, 4936, 188, 243, 96, 29, 32, 964, 8033, 47, 787, 30, 213, 3, 88, 225, 2447, 1280, 21768, 29, 16016, 15, 40, 4480, 11968, 29, 15, 91, 18, 23058, 3038, 3612, 1536, 372, 18, 5058, 9101, 739, 272, 12499, 30, 112, 13786, 209, 5695, 5, 4480, 11968, 29, 15, 47, 586, 189, 11, 272, 12499, 968, 189, 6, 24184, 13, 3, 9, 5834, 24809, 30, 112, 804, 14941, 68, 6264, 1222, 8, 126, 287, 49, 96, 12416, 3, 9, 207, 613, 11, 27, 737, 31, 17, 1280, 11808, 130, 615, 63, 13, 21945, 31, 7, 4974, 274, 18002, 227, 20495, 1625, 11, 13016, 157, 4470, 35, 2369, 80, 18, 8264, 16, 804, 1032, 6, 11, 70, 3315, 4283, 12, 36, 168, 5710, 38, 8, 1131, 2948, 4838, 34, 28, 8, 4294, 190, 167, 13, 18002, 5, 621, 8, 166, 3154, 6, 7963, 2235, 47, 2177, 6, 28, 20495, 1625, 11, 13016, 157, 4470, 35, 28503, 376, 45, 13450, 6, 113, 263, 3, 9, 6202, 44, 8, 804, 2752, 30, 112, 166, 14941, 5, 299, 13450, 6024, 112, 200, 21, 336, 6, 10391, 16, 334, 2393, 13, 112, 804, 3332, 6, 12, 3853, 7963, 2235, 57, 131, 3, 11739, 4013, 7549, 7, 227, 8, 2968, 141, 91, 18, 18789, 376, 1019, 1032, 11, 16, 8, 166, 18002, 2363, 5, 20495, 1625, 11902, 26, 3, 9, 6202, 44, 8, 804, 2752, 30, 112, 336, 14941, 6, 68, 8, 2827, 19, 24, 28, 8, 6813, 44, 3, 12100, 2517, 7549, 7, 12, 13450, 132, 47, 1327, 3, 88, 228, 43, 612, 5, 37, 6813, 6490, 11808, 33, 3960, 7, 21, 8, 1964, 6, 237, 3, 99, 21945, 54, 36, 1644, 12, 3292, 135, 5, 20495, 1625, 243, 10, 96, 3612, 7, 17, 215, 62, 130, 182, 1101, 16, 8, 1964, 11, 27, 317, 62, 33, 16, 207, 2346, 21, 5721, 5, 101, 56, 653, 12, 428, 135, 3, 9, 614, 97, 535, 4480, 11968, 29, 15, 31, 7, 4537, 7, 21, 112, 1907, 3407, 5695, 130, 623, 45, 1523, 3, 18, 3, 88, 163, 435, 91, 3, 88, 47, 8191, 30, 2721, 116, 3, 4936, 188, 6659, 10126, 28989, 71, 14061, 32, 73, 5616, 250, 13, 3, 9, 4335, 3, 6520, 14399, 16, 112, 1450, 8420, 44, 8, 166, 1964, 13, 8, 774, 16, 2051, 192, 1274, 977, 5, 37, 21768, 29, 24188, 141, 12, 3971, 8521, 45, 3411, 6, 213, 3, 88, 141, 118, 2505, 16, 8, 2011, 13786, 443, 3, 88, 10879, 132, 6, 11, 4363, 16, 29656, 163, 716, 274, 166, 1032, 30, 1701, 5, 216, 92, 141, 3, 9, 1256, 804, 1032, 6, 3586, 66, 68, 8, 804, 2893, 13, 8, 2363, 250, 13, 3, 9, 387, 7653, 5, 272, 12499, 47, 16403, 16, 8, 166, 18002, 2363, 6, 68, 4480, 11968, 29, 15, 3, 102, 15437, 376, 57, 3, 11739, 4389, 7549, 7, 116, 34, 1052, 15, 26, 5, 37, 12371, 1201, 18, 1490, 243, 10, 96, 196, 2124, 227, 4981, 27, 141, 882, 1126, 4974, 12, 9101, 739, 11, 27, 2124, 3, 99, 27, 3798, 3, 9, 385, 720, 27, 228, 2087, 1921, 376, 11, 237, 91, 18, 11433, 4921, 376, 11, 24, 19, 125, 65, 2817, 5, 96, 683, 35, 739, 19, 3, 9, 182, 207, 15705, 21, 140, 250, 3, 88, 19, 3, 9, 296, 6336, 11, 3, 88, 19, 168, 801, 12, 8, 372, 78, 27, 183, 182, 7035, 28, 8, 18002, 535, 272, 12499, 6, 113, 47, 3, 12100, 7549, 7, 16403, 145, 4480, 11968, 29, 15, 16, 8, 166, 2363, 6, 25932, 13, 147, 849, 49, 30, 112, 804, 661, 16, 8, 511, 10, 96, 2247, 536, 47, 125, 27, 47, 11873, 5, 1593, 357, 3, 88, 410, 3, 9, 207, 613, 11, 27, 737, 31, 17, 5, 4242, 6, 182, 207, 613, 5, 101, 2124, 149, 1704, 3, 88, 47, 535, 37, 15202, 126, 23458, 18002, 358, 47, 19346, 21, 48, 1964, 3, 3565, 2323, 10601, 44, 8, 166, 1964, 16, 2051, 12, 281, 223, 12, 8, 1230, 358, 5, 3, 4936, 188, 2753, 3966, 304, 26, 17, 243, 2283, 30, 1856, 24, 3, 88, 96, 4025, 17, 34, 1316, 12, 428, 126, 18002, 80, 72, 1253, 1686, 2651, 10, 96, 1326, 619, 16, 3, 9, 296, 213, 132, 19, 396, 231, 147, 6363, 535, 37, 358, 1279, 30, 8, 1873, 13, 10623, 95, 8, 8634, 3, 9, 385, 3, 18, 5205, 1547, 31, 7, 27287, 32, 1915, 457, 3492, 95, 91, 13, 1102, 16, 507, 189, 286, 227, 8, 372, 1817, 10379, 83, 920, 8, 9398, 13, 112, 804, 661, 6, 3140, 376, 59, 631, 97, 12, 743, 34, 274, 8, 23458, 6702, 97, 26, 376, 91, 5, 299, 34, 56, 369, 16, 21, 72, 12334, 38, 3, 9, 741, 13, 2136, 13, 1463, 1041, 44, 8, 414, 13, 284, 2363, 5, 290, 130, 386, 676, 44, 8, 414, 13, 8, 166, 2363, 28, 150, 2948, 30, 8, 4558, 6, 11, 8, 414, 13, 8, 511, 2363, 47, 3, 9, 1126, 14832, 3, 7, 1169, 115, 5, 3462, 80, 443, 3, 18, 23695, 454, 83, 157, 11063, 31, 7, 5205, 1547, 3, 18, 47, 91, 30, 8, 1463, 28, 1296, 676, 12, 281, 5, 37, 192, 6060, 2948, 410, 281, 91, 16, 8, 804, 386, 676, 68, 130, 641, 190, 12, 1593, 519, 11, 78, 1327, 47, 44, 8474, 5, 37, 2323, 33, 1338, 28, 304, 26, 17, 11, 377, 536, 1328, 7930, 29017, 16208, 2482, 3009, 30, 1771, 44, 18472, 415, 97, 12, 2204, 30, 125, 12, 103, 28, 18002, 21, 1], [21603, 10, 1079, 8200, 272, 6203, 6, 3, 14034, 13, 5641, 40, 26, 53, 6, 9884, 5718, 6, 68, 230, 840, 16, 1524, 6, 8519, 3, 9, 792, 13, 1630, 3991, 6, 379, 192, 12052, 13, 16, 221, 75, 4392, 28, 3, 9, 861, 5, 37, 3, 3708, 18, 1201, 18, 1490, 19, 11970, 13, 3, 25345, 8, 10883, 2319, 344, 1332, 16583, 11, 1797, 9975, 5, 1363, 272, 6203, 177, 725, 66, 8, 3991, 5, 12254, 5648, 15, 6, 23489, 53, 6, 1219, 8, 12730, 24, 8, 20265, 13, 6949, 5384, 130, 263, 57, 263, 57, 662, 5069, 15524, 2366, 11, 1341, 12, 116, 1363, 272, 6203, 47, 3, 9, 3, 7, 3422, 17, 2488, 16, 1013, 9884, 5718, 11, 10096, 5718, 5, 96, 634, 11819, 845, 1327, 13, 24, 1843, 2817, 344, 2448, 11, 66, 175, 1742, 5, 216, 845, 79, 33, 66, 22717, 53, 70, 3744, 11, 5188, 7797, 976, 243, 8667, 5648, 15, 5, 37, 23489, 127, 7760, 1363, 272, 6203, 5374, 80, 627, 215, 625, 12, 112, 234, 1772, 376, 8, 1253, 12, 320, 44, 10453, 4852, 263, 44, 3, 7, 3422, 17, 15926, 68, 258, 3217, 376, 5569, 29, 16587, 4852, 5, 451, 1219, 8, 12730, 24, 8, 4940, 47, 258, 6949, 120, 3, 28493, 3140, 376, 11319, 11, 3, 89, 3535, 4632, 5, 8667, 5648, 15, 243, 10, 96, 634, 15524, 288, 31, 7, 3, 60, 3297, 12252, 19, 24, 30, 3, 9, 381, 13, 9194, 6949, 6775, 133, 1837, 28, 8, 11819, 893, 16, 8, 11819, 31, 7, 443, 42, 16, 112, 12268, 535, 451, 1219, 8, 12730, 3, 9, 511, 4940, 47, 1026, 57, 1363, 272, 6203, 21, 3, 9, 1851, 16, 1524, 44, 8, 1246, 13, 1179, 42, 968, 11, 227, 3644, 11943, 7, 3, 88, 47, 865, 6949, 120, 3, 28493, 5, 8667, 5648, 15, 243, 192, 5234, 45, 8, 5641, 40, 26, 53, 563, 141, 92, 263, 11244, 13, 271, 6949, 120, 3, 28493, 5, 37, 12730, 65, 118, 1219, 24, 1363, 272, 6203, 47, 16, 8, 3, 27380, 274, 3122, 38, 3, 9, 9884, 5718, 5076, 5502, 344, 16164, 11, 15041, 5, 37, 3689, 6, 84, 19, 1644, 12, 336, 192, 1274, 6, 3256, 5, 1], [21603, 10, 18027, 11, 871, 130, 29766, 26, 45, 7112, 9, 107, 8020, 9, 2833, 30, 2875, 227, 3, 9, 388, 4281, 1058, 44, 8, 5998, 16026, 12, 4279, 2448, 11, 717, 5, 6027, 7, 130, 16163, 12, 17405, 28, 8, 388, 6, 3, 9, 1021, 2095, 5502, 5, 3, 15944, 2279, 24, 8, 3, 8715, 388, 141, 1026, 633, 151, 2290, 545, 9193, 12153, 5, 37, 5752, 7792, 13, 7112, 9, 107, 8020, 9, 2833, 6, 4603, 18075, 23, 3695, 17, 11158, 9, 63, 6, 113, 47, 29766, 26, 45, 8, 3064, 6, 243, 24, 132, 141, 118, 96, 29, 32, 2290, 545, 15093, 7, 1686, 2651, 24, 8, 388, 47, 96, 138, 782, 16, 8, 562, 1280, 707, 3695, 17, 11158, 9, 63, 243, 24, 8, 388, 141, 118, 4281, 3, 8118, 23, 9, 3929, 1058, 21, 8, 657, 192, 203, 5, 216, 243, 24, 8, 2833, 141, 3150, 5776, 3, 9, 934, 3, 17211, 24, 8, 388, 225, 59, 36, 10548, 12, 2331, 3, 9, 4740, 5, 96, 12146, 7, 20507, 47, 1026, 550, 976, 707, 3695, 17, 11158, 9, 63, 243, 6, 2651, 24, 8, 4740, 16, 8, 5502, 31, 7, 8244, 30, 2875, 47, 59, 112, 4683, 20507, 5, 37, 5415, 639, 18905, 7012, 16, 20958, 826, 633, 6032, 16, 3, 17208, 844, 6, 379, 8, 18443, 12710, 30, 8, 14317, 9, 706, 13442, 30, 368, 2929, 31, 7, 11566, 84, 646, 6352, 151, 3654, 5, 1], [21603, 10, 6308, 15, 1699, 4331, 32, 530, 8, 4462, 653, 28, 8, 336, 888, 13, 8, 467, 6, 826, 2283, 19396, 7, 57, 4409, 6343, 7, 291, 32, 6, 1027, 11849, 1699, 1304, 739, 11, 10243, 12455, 440, 5667, 76, 5, 12749, 29, 986, 2216, 348, 11, 7153, 17, 106, 216, 7820, 17, 530, 3, 9, 653, 16, 893, 985, 21, 8, 10282, 7, 5, 18588, 3217, 623, 4784, 2793, 16, 4963, 38, 79, 808, 610, 13, 3, 9, 21124, 1588, 16, 8, 511, 1059, 5, 1210, 3763, 3, 26705, 4463, 7, 989, 1891, 3, 9, 5695, 12, 579, 1840, 377, 17279, 152, 18, 7473, 3556, 9, 969, 3, 3108, 2067, 1824, 400, 1823, 23, 63, 2551, 1967, 32, 6, 11, 2050, 5104, 6393, 1047, 3666, 45, 307, 18, 1987, 2871, 6, 298, 8, 10282, 7, 1891, 166, 3511, 13, 8, 774, 12, 3, 3108, 71, 1361, 3004, 210, 11, 7970, 49, 15334, 23, 32, 17, 374, 15, 5, 18588, 1513, 7970, 49, 5192, 3038, 7754, 10666, 12, 46, 778, 8173, 2871, 68, 808, 2337, 13, 70, 166, 1666, 116, 2158, 651, 205, 109, 4102, 4918, 1054, 147, 3, 9, 10736, 30, 586, 676, 5, 94, 808, 997, 676, 21, 3, 9, 1028, 1927, 2429, 26, 467, 12, 1759, 3, 9, 653, 38, 1138, 60, 40, 1266, 3600, 302, 3, 20317, 15, 26, 45, 885, 620, 11, 2216, 348, 5241, 112, 194, 147, 21, 9637, 304, 162, 63, 12, 5755, 3, 18, 2199, 34, 47, 8, 6081, 31, 7, 336, 6275, 38, 3, 88, 3, 31779, 28, 3, 9, 5738, 2871, 10545, 15627, 5, 18588, 10056, 223, 116, 6343, 7, 291, 32, 10719, 147, 45, 3, 9, 8394, 954, 83, 30, 3097, 676, 21, 205, 109, 4102, 12, 5755, 5, 299, 8, 10282, 7, 593, 1361, 44, 335, 4536, 274, 985, 18, 715, 116, 1823, 23, 63, 2551, 1967, 32, 47, 4459, 18, 6043, 15, 26, 21, 46, 22142, 8000, 30, 3004, 210, 11, 304, 162, 63, 4918, 1054, 8, 514, 1288, 5, 37, 2692, 228, 59, 143, 8, 167, 13, 70, 80, 18, 348, 2337, 227, 8, 1733, 38, 70, 3505, 3476, 583, 135, 12537, 120, 5, 94, 47, 18588, 31, 7, 8453, 351, 24, 3217, 116, 4794, 25594, 31, 7, 1733, 2237, 12, 3, 9, 710, 18, 5517, 2604, 45, 21615, 6377, 1699, 1304, 739, 6, 12069, 57, 205, 109, 4102, 5, 374, 2780, 288, 1699, 4331, 32, 47, 8, 511, 234, 1959, 12, 36, 3731, 18, 115, 14029, 26, 6, 30, 3, 3891, 676, 6, 68, 541, 8, 24499, 263, 659, 13, 34, 38, 3709, 3, 3108, 12455, 440, 5667, 76, 6, 3, 9, 11474, 45, 8, 6788, 6, 3, 102, 7906, 26, 12, 20, 89, 17, 120, 8093, 190, 3, 9, 3, 4076, 4733, 1996, 21, 46, 3, 32, 102, 1493, 202, 343, 653, 5, 37, 10282, 7, 530, 223, 441, 11214, 620, 28, 128, 1287, 3334, 5834, 3, 3131, 216, 7820, 17, 147, 73, 28236, 3843, 227, 9455, 676, 5, 611, 6, 1699, 4331, 32, 1632, 3731, 687, 18, 24346, 18, 7, 9, 77, 17, 38, 3, 88, 530, 30, 8, 414, 13, 430, 1231, 8394, 954, 83, 12, 3807, 112, 596, 8, 996, 500, 28, 8, 336, 888, 13, 8, 467, 6, 205, 109, 4102, 3, 21049, 5, 10282, 7, 2090, 13, 22209, 5225, 29, 6193, 243, 10, 96, 1326, 31, 60, 10978, 12, 43, 1513, 68, 69, 821, 47, 3, 9, 418, 394, 784, 6736, 581, 312, 77, 1370, 908, 11, 8, 467, 228, 43, 2767, 893, 194, 5, 96, 5110, 29178, 396, 186, 6854, 1187, 8, 14667, 440, 583, 178, 3, 9, 248, 1154, 6, 713, 45, 213, 62, 130, 3, 9, 21, 17, 7602, 977, 16, 14112, 69, 161, 2206, 11, 3667, 47, 1287, 5, 96, 196, 17, 47, 914, 3505, 3476, 45, 1742, 1187, 8, 14667, 440, 24, 583, 178, 1057, 1102, 6, 34, 31, 7, 59, 15721, 2056, 3, 18, 79, 130, 2024, 16, 149, 79, 1944, 11, 62, 141, 3, 9, 360, 6854, 6, 24, 47, 8, 1750, 535, 18588, 24499, 10, 2158, 651, 21512, 6, 2067, 1824, 400, 1823, 23, 63, 2551, 1967, 32, 6, 5104, 6393, 1047, 6, 27495, 5225, 109, 6, 5531, 6193, 6, 2158, 651, 205, 109, 4102, 6, 13375, 739, 10498, 117, 5104, 432, 152, 6, 5192, 2143, 7754, 10666, 6, 1027, 11849, 1699, 1304, 739, 6, 5376, 26085, 41, 4010, 17, 201, 4972, 205, 23434, 7, 6, 11560, 107, 21581, 6, 4409, 6343, 7, 291, 32, 6, 7124, 282, 88, 5, 21438, 7, 10, 7566, 1744, 7, 4972, 6, 16637, 18707, 76, 63, 9, 4796, 7396, 6, 4794, 1839, 15525, 6, 11859, 2737, 739, 6, 6308, 15, 1699, 4331, 32, 6, 4794, 25594, 6, 3, 26705, 14046, 6, 10243, 12455, 440, 5667, 76, 5, 10282, 7, 10, 7291, 19191, 6, 7153, 17, 106, 216, 7820, 17, 6, 9616, 15811, 109, 6, 7124, 16700, 6, 71, 1361, 3004, 210, 6, 9637, 304, 162, 63, 6, 1138, 60, 40, 1266, 3600, 302, 117, 7254, 159, 11469, 9789, 362, 6, 15334, 23, 32, 17, 374, 15, 6, 4027, 157, 12551, 6, 7486, 4737, 2091, 6, 12749, 29, 986, 2216, 348, 41, 4010, 17, 201, 9765, 18305, 6, 20310, 1839, 26, 26, 6, 4857, 7714, 5, 21438, 7, 10, 11092, 63, 7, 10295, 1306, 6, 8188, 5312, 6, 3926, 202, 11595, 6, 9771, 4712, 60, 10217, 6, 5424, 1896, 29345, 7, 6, 3, 11748, 6193, 6, 12707, 28571, 6, 7486, 4972, 5, 1], [21603, 10, 3, 31873, 30003, 24187, 32, 18, 188, 40, 624, 457, 6, 12074, 47, 4792, 11, 430, 388, 7532, 116, 46, 9835, 71, 519, 10056, 135, 16, 5500, 1544, 1483, 1592, 2409, 44, 3, 3076, 10, 1458, 22866, 30, 1856, 5, 4738, 676, 274, 8, 8420, 8, 443, 47, 16, 1524, 2409, 6, 5811, 63, 2029, 6, 116, 3, 9, 19080, 3424, 144, 8029, 1599, 26, 28, 3, 9, 2195, 5, 5076, 241, 12, 8320, 18050, 8688, 6, 14141, 113, 79, 497, 65, 2416, 12, 8, 9835, 5, 37, 443, 47, 13876, 44, 8, 3112, 5, 283, 7, 24187, 32, 18, 188, 40, 624, 457, 3977, 45, 1317, 5157, 6, 3, 9, 442, 18, 2528, 3524, 6498, 435, 5, 465, 10319, 7, 43, 118, 263, 38, 780, 6, 2095, 243, 5, 283, 7, 24187, 32, 18, 188, 40, 624, 457, 47, 6597, 44, 160, 2039, 31, 7, 234, 16, 5500, 1544, 1483, 1592, 2409, 5, 451, 47, 2170, 16, 25101, 11, 141, 4114, 16, 1524, 21, 1179, 203, 6, 9938, 1524, 17021, 3121, 15, 189, 10759, 969, 243, 5, 486, 8, 97, 13, 8, 8420, 6, 255, 47, 30, 160, 194, 12, 161, 16, 3, 9, 1595, 5, 37, 3048, 13, 8, 2601, 1190, 6, 84, 47, 18710, 6780, 16, 8, 8420, 6, 43, 118, 3641, 5, 20294, 43, 118, 646, 44, 8, 353, 16, 14886, 12, 8, 7584, 5, 71, 2493, 45, 160, 4284, 8595, 2922, 83, 24187, 32, 18, 188, 40, 624, 457, 243, 10, 96, 7008, 384, 65, 141, 165, 842, 12, 52, 29, 91, 6, 44, 48, 1619, 97, 6, 62, 56, 470, 36, 8, 337, 541, 5, 96, 7638, 1701, 706, 62, 130, 544, 38, 3, 9, 384, 28, 3, 31873, 1338, 160, 6164, 2170, 23213, 11, 3, 6955, 21, 1619, 5, 96, 196, 336, 1509, 160, 7267, 38, 255, 646, 12, 281, 12, 161, 30, 1856, 1379, 6, 68, 4413, 865, 27, 47, 3609, 160, 609, 38, 255, 2804, 550, 16, 8, 2815, 535, 2973, 2685, 115, 53, 8, 8420, 38, 96, 107, 127, 52, 3286, 121, 374, 17, 86, 7, 102, 14626, 25568, 6, 243, 10, 96, 634, 384, 33, 31521, 5, 37, 2594, 13, 48, 1254, 924, 1687, 56, 36, 28, 135, 284, 97, 79, 1175, 70, 234, 5, 96, 634, 2535, 21761, 8, 3112, 10755, 53, 8, 7592, 9835, 6, 84, 47, 18710, 6780, 5, 96, 1326, 33, 479, 12, 2516, 12, 1363, 18050, 8688, 16, 4689, 12, 48, 16345, 535, 37, 11696, 18, 1201, 18, 1490, 388, 7532, 44, 8, 2601, 1190, 3048, 16, 3, 9, 2404, 1706, 16, 2833, 298, 8, 1706, 13, 8, 2838, 18, 1201, 18, 1490, 2535, 13, 8, 19080, 19, 230, 5711, 5, 1], [21603, 10, 21768, 29, 21074, 15782, 155, 23, 15, 3977, 227, 3, 9, 16345, 28, 3, 9, 2340, 15214, 383, 15575, 31, 7, 21689, 18, 1326, 4911, 397, 51, 1964, 5, 37, 944, 18, 1201, 18, 1490, 47, 1560, 57, 8, 2340, 15214, 227, 633, 14149, 764, 323, 16, 3, 9, 8420, 38, 8, 1964, 2804, 190, 8390, 1410, 5, 96, 634, 711, 807, 369, 116, 2948, 42, 2340, 15214, 7, 43, 12, 1903, 8, 158, 3171, 106, 11, 1903, 14149, 976, 2271, 5643, 31, 7, 11768, 15, 243, 5, 96, 11880, 19, 8, 4431, 962, 62, 31, 60, 479, 139, 5, 96, 7238, 31, 7, 3, 9, 418, 13, 2340, 15214, 7, 16, 11, 300, 8, 1964, 823, 34, 36, 7724, 21, 1424, 6, 16707, 42, 2095, 2340, 15214, 7, 5, 96, 1570, 792, 132, 31, 7, 300, 943, 2340, 15214, 7, 24, 161, 30, 284, 1964, 5, 96, 1326, 31, 162, 530, 3, 9, 14149, 7021, 11, 62, 31, 60, 1107, 544, 12, 317, 13, 3, 9, 360, 912, 6, 823, 62, 2468, 3, 9, 1634, 2006, 30, 149, 1006, 79, 54, 147, 4914, 178, 5, 96, 134, 9, 63, 62, 474, 3, 9, 335, 3, 20656, 7, 399, 1781, 2006, 30, 34, 6, 3, 99, 62, 31, 60, 352, 943, 157, 102, 107, 79, 31, 60, 163, 2225, 12, 1903, 178, 1640, 157, 102, 107, 42, 424, 114, 24, 535, 15782, 155, 23, 15, 6, 113, 47, 7494, 21, 8, 6834, 63, 18, 6221, 7041, 372, 6, 47, 1026, 12, 2833, 16, 27389, 68, 3977, 865, 5, 37, 2600, 31, 7, 3, 19585, 643, 6, 8, 412, 3597, 6, 243, 34, 133, 576, 18, 18140, 17, 15, 28, 66, 2193, 5779, 16, 46, 4962, 139, 8, 5415, 5, 37, 4751, 6400, 75, 3350, 7, 31, 2125, 41, 4184, 188, 61, 4683, 3, 9, 2493, 3558, 125, 133, 36, 612, 12, 1172, 1455, 5, 3, 4868, 15782, 155, 23, 15, 31, 7, 1687, 6, 18537, 12, 1373, 8191, 56, 1049, 8, 337, 845, 11768, 15, 6, 113, 65, 118, 12166, 16, 5245, 13992, 13, 374, 276, 4515, 1964, 16, 15575, 5, 96, 188, 7, 1116, 38, 24, 3282, 13, 2971, 7560, 7, 139, 39, 809, 11, 25, 456, 1631, 13, 378, 24, 228, 1837, 6, 24, 31, 7, 116, 25, 31, 60, 103, 32, 2726, 12, 5124, 976, 3, 88, 1219, 9938, 10256, 3349, 5, 96, 5801, 25, 456, 1631, 81, 22298, 11, 8, 7702, 11, 125, 228, 6149, 1837, 258, 25, 31, 60, 470, 352, 12, 36, 44, 8, 851, 13, 8, 158, 3171, 106, 11, 25, 31, 60, 470, 352, 12, 1369, 136, 10879, 535, 86, 3, 9, 2450, 5415, 6, 430, 21768, 29, 21074, 6, 878, 152, 499, 1725, 88, 49, 6, 12889, 3977, 16, 2833, 227, 5706, 3, 9, 842, 3211, 383, 8, 166, 1726, 13, 8, 23679, 15, 11879, 1331, 16, 2487, 7, 2617, 5, 1], [21603, 10, 9982, 10169, 152, 6, 13597, 1219, 9938, 3349, 3, 88, 96, 1608, 217, 8, 8619, 689, 121, 227, 3, 30846, 3, 22052, 342, 6476, 27588, 7, 16, 1882, 6, 68, 56, 59, 10505, 112, 1205, 5, 37, 2968, 4785, 8, 1412, 1150, 3802, 826, 223, 3730, 24, 2697, 376, 91, 21, 3, 9, 215, 6, 11, 3, 7, 144, 91, 2430, 1421, 250, 13, 3, 9, 1028, 5133, 920, 6476, 4010, 5, 216, 243, 10, 96, 196, 17, 19, 2437, 19367, 12, 1845, 24, 535, 9982, 10169, 152, 56, 59, 36, 1400, 21, 8, 456, 13, 8, 6552, 3815, 774, 44, 25080, 30, 586, 1660, 68, 243, 112, 3938, 97, 19, 230, 271, 8413, 16, 96, 8041, 7, 121, 1066, 145, 767, 5, 216, 1219, 9938, 3349, 10, 96, 196, 17, 19, 310, 614, 373, 12, 1590, 11, 2870, 39, 194, 223, 5, 148, 473, 207, 11, 473, 1065, 6, 258, 25, 129, 8, 416, 4583, 5, 96, 634, 6025, 294, 19, 1187, 140, 230, 5, 27, 241, 12, 473, 1065, 116, 27, 183, 1540, 223, 5, 27, 241, 12, 473, 1346, 11, 4881, 5, 27, 278, 31, 17, 809, 3, 99, 34, 19, 192, 1274, 42, 1296, 535, 9982, 10169, 152, 263, 627, 3179, 7, 11, 5799, 874, 1766, 16, 112, 5695, 774, 21, 896, 826, 112, 3996, 1755, 51, 888, 45, 1491, 26165, 24599, 5, 216, 19, 10876, 12, 129, 30, 8, 1057, 541, 11, 47, 8686, 44, 8, 1886, 31, 7, 314, 2292, 1369, 147, 2977, 12033, 16, 3, 9, 554, 18, 9476, 467, 16, 3144, 4975, 30, 2875, 5, 3440, 1276, 102, 12899, 23578, 65, 263, 874, 126, 8097, 7, 641, 48, 1248, 11, 3256, 12, 43, 46, 1046, 16, 22358, 1039, 5104, 159, 28736, 11, 25258, 31, 7, 11478, 9928, 283, 115, 3096, 15, 5, 9982, 10169, 152, 243, 10, 96, 9546, 51, 159, 51, 21, 8, 774, 19, 600, 5, 94, 19, 1450, 6, 1728, 5, 96, 1326, 1800, 24, 336, 215, 38, 168, 68, 34, 47, 3, 9, 1551, 126, 351, 21, 66, 13, 178, 5, 101, 214, 8, 6552, 3815, 3, 9, 720, 72, 230, 11, 54, 31, 17, 1749, 21, 8, 774, 12, 456, 535, 896, 743, 70, 386, 18, 19515, 1552, 13, 8, 907, 1323, 581, 31857, 16, 17659, 30, 1856, 5, 14373, 2743, 12923, 13228, 15, 243, 2283, 48, 471, 3, 88, 410, 59, 473, 31857, 130, 5191, 26, 57, 8, 337, 2443, 38, 112, 293, 596, 6, 896, 11, 9145, 907, 5, 17740, 7, 43, 141, 8, 2337, 16, 70, 1100, 4677, 28, 896, 6, 3447, 386, 11, 5364, 80, 13, 70, 336, 662, 6552, 3815, 1031, 5, 275, 9982, 10169, 152, 317, 7, 79, 33, 3, 9, 779, 5888, 5, 216, 243, 10, 96, 3696, 17, 324, 1483, 33, 3, 9, 248, 372, 5, 328, 43, 8, 869, 13, 3370, 5, 328, 43, 1021, 1566, 1508, 5, 421, 351, 336, 774, 1267, 34, 19, 310, 3429, 12, 3853, 135, 5, 96, 10273, 33, 310, 14209, 12, 577, 581, 5, 96, 196, 183, 1134, 417, 6, 237, 3, 99, 79, 56, 59, 497, 34, 8002, 6, 8, 151, 113, 214, 8, 6552, 3815, 214, 31857, 33, 1728, 3, 9, 18766, 21, 8, 2233, 535, 1], [21603, 10, 37, 8420, 2817, 81, 10668, 10, 1755, 22866, 44, 8, 23704, 13, 8, 71, 22367, 11, 24583, 2409, 16, 312, 9031, 18, 106, 18, 134, 15, 9, 6, 25223, 5, 37, 388, 6, 113, 2095, 243, 19, 9742, 16, 112, 460, 7, 6, 47, 4260, 44, 8, 3112, 21, 3, 9, 819, 2871, 11, 18024, 1317, 16898, 7, 6, 8, 25102, 313, 243, 5, 216, 47, 799, 9253, 15, 26, 12, 8, 3671, 1524, 4457, 21, 856, 1058, 5, 37, 1013, 989, 18, 6115, 24124, 1343, 13, 8, 71, 22367, 47, 3168, 21, 81, 1296, 716, 298, 2095, 4468, 70, 2332, 20182, 5, 71, 5468, 7, 15893, 21, 25223, 5076, 243, 34, 47, 59, 487, 1670, 12, 856, 38, 48, 97, 38, 8, 96, 15601, 23, 122, 257, 19, 230, 271, 4468, 57, 8, 3857, 2823, 1280, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[7433, 18, 413, 2673, 33, 6168, 640, 8, 12580, 17600, 7, 11, 970, 51, 89, 2593, 11, 10987, 32, 1343, 227, 18368, 2953, 57, 16133, 4937, 5, 1], [2759, 8548, 14264, 43, 118, 10932, 57, 1472, 16, 3, 9, 18024, 1584, 739, 3211, 16, 27874, 690, 2050, 5, 1], [9765, 13450, 5536, 15, 26, 12, 11148, 1102, 44, 8, 29656, 2698, 12942, 2177, 13, 11808, 372, 18, 5058, 23695, 7963, 2235, 5, 1], [71, 1798, 9884, 5718, 5076, 5502, 4006, 91, 3, 9, 939, 13, 3, 7, 994, 6032, 30, 5234, 6, 3, 9, 12730, 44, 9884, 15633, 2243, 47, 1219, 5, 1], [389, 3, 8715, 388, 113, 13249, 2448, 139, 3, 9, 562, 44, 3, 9, 3, 8118, 23, 9, 3929, 2833, 16, 20958, 65, 3492, 112, 5888, 12, 5781, 2448, 6, 15423, 783, 934, 5, 1], [3, 16196, 9303, 749, 2122, 6336, 7, 18588, 24499, 2182, 5402, 3, 9, 1480, 4023, 18, 2700, 6224, 147, 8, 10282, 7, 3, 3565, 3, 9, 2290, 13, 14101, 15, 15, 7, 11, 192, 4459, 2190, 5, 1], [71, 388, 28, 2416, 12, 3, 9, 443, 24, 47, 1381, 16, 3, 9, 12699, 2601, 1190, 8420, 16, 3414, 1524, 19, 271, 7803, 57, 2095, 5, 1], [22982, 21074, 12020, 11768, 15, 845, 1112, 12, 8, 2600, 398, 36, 263, 826, 8, 1687, 13, 3, 25742, 15782, 155, 23, 15, 5, 1], [9145, 896, 2076, 1846, 49, 802, 18075, 9982, 10169, 152, 845, 34, 65, 118, 19367, 3429, 12, 8269, 3, 9, 1025, 779, 2871, 5, 1], [71, 3, 1927, 6938, 65, 118, 1560, 57, 46, 73, 16376, 2095, 443, 16523, 12, 46, 3583, 580, 6, 3140, 376, 28, 96, 7, 49, 2936, 280, 18, 12757, 5157, 1280, 1]]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function works with one or several examples. In the case of several examples,\n",
    "# the tokenizer will return a list of lists for each key:\n",
    "preprocess_function(raw_datasets[\"train\"][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868b2e79",
   "metadata": {},
   "source": [
    "To apply this function on all the pairs of sentences in our dataset, we just use the map method of our dataset object we created earlier. This will apply the function on all the elements of all the splits in dataset, so our training, validation and testing data will be preprocessed in one single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f19e1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/jvdboss/workspace/Testing_Stuff/summarization_tensorflow/final_data/train/cache-91d5b0c8d787a43b.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/jvdboss/workspace/Testing_Stuff/summarization_tensorflow/final_data/validation/cache-88923bf6342ea0ed.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/jvdboss/workspace/Testing_Stuff/summarization_tensorflow/final_data/test/cache-e706f6542e2ada3f.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e516bebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/jvdboss/workspace/Testing_Stuff/summarization_tensorflow/final_data/train/cache-0689c88f462d5410.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/jvdboss/workspace/Testing_Stuff/summarization_tensorflow/final_data/validation/cache-876281ac29164311.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/jvdboss/workspace/Testing_Stuff/summarization_tensorflow/final_data/test/cache-036ab76e8634c258.arrow\n"
     ]
    }
   ],
   "source": [
    "# let's take the sample of train, validation and test dataset \n",
    "# If you have enough memory and resources you can use the entire data for the project.\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(6000))\n",
    "small_validation_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(1000))\n",
    "small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34a7054",
   "metadata": {},
   "source": [
    "---\n",
    "Now that our data is ready, we can download the pretrained model and fine-tune it. Since our task is sequence-to-sequence (both the input and output are text sequences), we use the AutoModelForSeq2SeqLM class. Like with the tokenizer, the from_pretrained method will download and cache the model for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53f227cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-14 18:03:13.584861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 18:03:13.730848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 18:03:13.731013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 18:03:13.731923: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-14 18:03:13.733125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 18:03:13.733356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 18:03:13.733541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 18:03:14.364393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 18:03:14.364577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 18:03:14.364712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 18:03:14.364824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2592 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed2aabe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define the cosntants \n",
    "batch_size = 1\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "num_train_epochs = 1\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "push_to_hub_model_id = f\"{model_name}-finetuned-xsum\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fb9e60",
   "metadata": {},
   "source": [
    "Then, we need a special kind of data collator, which will not only pad the inputs to the maximum length in the batch, but also the labels. Note that our data collators are multi-framework, so make sure you set return_tensors='tf' so you get tf.Tensor objects back and not something else!\n",
    "\n",
    "We also want to compute ROUGE metrics, which will require us to generate text from our model. To speed things up, we can compile our generation loop with XLA. This results in a huge speedup - up to 100X! The downside of XLA generation, though, is that it doesn't like variable input shapes, because it needs to run a new compilation for each new input shape! To compensate for that, let's use pad_to_multiple_of for the dataset we use for text generation. This will reduce the number of unique input shapes a lot, meaning we can get the benefits of XLA generation with only a few compilations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "723773f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")\n",
    "\n",
    "generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\", pad_to_multiple_of=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5338aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'summary', 'id', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 204045\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2bc91b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'summary', 'id', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 6000\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a539bd0",
   "metadata": {},
   "source": [
    "Next, we convert our datasets to tf.data.Dataset, which Keras understands natively. There are two ways to do this - we can use the slightly more low-level Dataset.to_tf_dataset() method, or we can use Model.prepare_tf_dataset(). The main difference between these two is that the Model method can inspect the model to determine which column names it can use as input, which means you don't need to specify them yourself. Make sure to specify the collator we just created as our collate_fn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6892779f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = model.prepare_tf_dataset(\n",
    "    small_train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "validation_dataset = model.prepare_tf_dataset(\n",
    "    small_validation_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "test_dataset = model.prepare_tf_dataset(\n",
    "    small_test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bce5c21",
   "metadata": {},
   "source": [
    "Now we initialize our loss and optimizer and compile the model. Note that most Transformers models compute loss internally - we can train on this as our loss value simply by not specifying a loss when we compile()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81318c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b228b3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_fn(eval_predictions):\n",
    "    predictions, labels = eval_predictions\n",
    "    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    for label in labels:\n",
    "        label[label < 0] = tokenizer.pad_token_id  # Replace masked label tokens\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_predictions = [\n",
    "        \"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_predictions\n",
    "    ]\n",
    "    decoded_labels = [\n",
    "        \"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels\n",
    "    ]\n",
    "    result = metric.compute(\n",
    "        predictions=decoded_predictions, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    # Add mean generated length\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eb6eb2",
   "metadata": {},
   "source": [
    "---\n",
    "### Train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "839ee652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 787s 129ms/step - loss: 2.9422 - val_loss: 2.6128\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f2e31e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [2.942176342010498], 'val_loss': [2.612797498703003]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e56c09",
   "metadata": {},
   "source": [
    "---\n",
    "### Evaluate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95b67f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 38s 38ms/step - loss: 2.5948\n",
      "2.594815731048584\n"
     ]
    }
   ],
   "source": [
    "# Let's check the loss of the model \n",
    "loss = model.evaluate(test_dataset)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f224a3",
   "metadata": {},
   "source": [
    "---\n",
    "### Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d00f8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the pretrained model\n",
    "model.save_pretrained(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c643210b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5e05a9",
   "metadata": {},
   "source": [
    "---\n",
    "### Taking custom input to check the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b34c7fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (539 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "document = 'The full cost of damage in Newton Stewart, one of the areas worst affected, is still being assessed.\\nRepair work is ongoing in Hawick and many roads in Peeblesshire remain badly affected by standing water.\\nTrains on the west coast mainline face disruption due to damage at the Lamington Viaduct.\\nMany businesses and householders were affected by flooding in Newton Stewart after the River Cree overflowed into the town.\\nFirst Minister Nicola Sturgeon visited the area to inspect the damage.\\nThe waters breached a retaining wall, flooding many commercial properties on Victoria Street - the main shopping thoroughfare.\\nJeanette Tate, who owns the Cinnamon Cafe which was badly affected, said she could not fault the multi-agency response once the flood hit.\\nHowever, she said more preventative work could have been carried out to ensure the retaining wall did not fail.\\n\"It is difficult but I do think there is so much publicity for Dumfries and the Nith - and I totally appreciate that - but it is almost like we\\'re neglected or forgotten,\" she said.\\n\"That may not be true but it is perhaps my perspective over the last few days.\\n\"Why were you not ready to help us a bit more when the warning and the alarm alerts had gone out?\"\\nMeanwhile, a flood alert remains in place across the Borders because of the constant rain.\\nPeebles was badly hit by problems, sparking calls to introduce more defences in the area.\\nScottish Borders Council has put a list on its website of the roads worst affected and drivers have been urged not to ignore closure signs.\\nThe Labour Party\\'s deputy Scottish leader Alex Rowley was in Hawick on Monday to see the situation first hand.\\nHe said it was important to get the flood protection plan right but backed calls to speed up the process.\\n\"I was quite taken aback by the amount of damage that has been done,\" he said.\\n\"Obviously it is heart-breaking for people who have been forced out of their homes and the impact on businesses.\"\\nHe said it was important that \"immediate steps\" were taken to protect the areas most vulnerable and a clear timetable put in place for flood prevention plans.\\nHave you been affected by flooding in Dumfries and Galloway or the Borders? Tell us about your experience of the situation and how it was handled. Email us on selkirk.news@bbc.co.uk or dumfries@bbc.co.uk.'\n",
    "if 't5' in model_name: \n",
    "    document = \"summarize: \" + document\n",
    "tokenized = tokenizer([document], return_tensors='np')\n",
    "out = model.generate(**tokenized, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5519d766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> The damage to the roads in the Scottish Borders has been a \"wonderful and aback\" for people who have been forced out of their homes, the Labour Party has said.</s>\n"
     ]
    }
   ],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    print(tokenizer.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcf1885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a281bf39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96c8ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d927da31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
