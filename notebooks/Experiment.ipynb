{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49774185",
   "metadata": {},
   "source": [
    "## Text Summarization Using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25963682",
   "metadata": {},
   "source": [
    "---\n",
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ebd04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.25.1\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM\n",
    "from transformers import AdamWeightDecay\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaf0eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets = raw_datasets.save_to_disk(\"/home/jvdboss/workspace/Testing_Stuff/summarization_tensorflow/final_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8674c39",
   "metadata": {},
   "source": [
    "---\n",
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0f2cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_from_disk(\"/home/jvdboss/workspace/Testing_Stuff/summarization_tensorflow/final_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "143fb73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 204045\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 11332\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 11334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check out the data \n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84effa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_samples(dataset, num_examples=5):\n",
    "    assert num_examples <= len(\n",
    "        dataset\n",
    "    ), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset) - 1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset) - 1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5c1ee61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Speaking to the BBC, the co-chair of an inquiry into BHS compared Sir Philip unfavourably to late media magnate Robert Maxwell, who took millions from the Mirror Group's pension funds.\\n\"This statement is highly defamatory and completely false,\" said law firm Schillings, representing Sir Philip.\\nMr Field said he would not apologise.\\nHe told BBC business editor Simon Jack that Sir Philip's conduct was \"displacement therapy\".\\n\"Instead of writing a big cheque he is firing off lawyers' letters. He needs to stop messing around and write a big cheque\".\\nHis earlier remarks were made on Radio 4's Today programme.\\nThe development came after a damning MPs' report, published on Monday, concluded Sir Philip, the billionaire former owner of BHS, extracted large sums and left the business on \"life support\".\\nThe report, from the Business, Innovation and Skills and Work and Pensions committees and co-chaired by Mr Field, did not suggest that Sir Phillip had done anything illegal.\\nBut it said his failure to resolve BHS's Â£571m pension deficit was a major factor in the firm's demise.\\nThe pension scheme is now in the Pension Protection Fund, meaning that its 20,000 members will receive less money than they had expected\\nSir Philip, responding late on Monday to the report, called it \"the predetermined and inaccurate output of a biased and unfair process\".\\nBHS is in the process of closing down after what the MPs' report called the \"shambolic\" ownership of Dominic Chappell, who bought the retail chain from Sir Philip for Â£1 last year.\\nBHS was \"hurriedly sold to a manifestly unsuitable\" buyer in that deal, even though Sir Philip knew Mr Chappell was a former bankrupt with no retail experience, the MPs found.\\nThe sale went unchallenged because Sir Philip ran his retail empire as \"a personal fiefdom\", they said.\\n\"With the benefit of hindsight, clearly Retail Acquisitions and Mr Chappell were a very bad choice as purchaser on many fronts and I feel badly let down,\" said Sir Philip.\\nBut he said the sale of BHS was \"made one hundred per cent in good faith\".\\n\"As I told the committees, I am trying to find a solution for the BHS pension and am continuing to work with the Regulator to achieve an outcome.\\n\"I am sad and sorry for all the BHS people caught up in this horrid story, but I do not believe that this story is being in any way fairly portrayed.\"\\nMr Field and Sir Philip have had a long-running feud, with the Monaco-based businessman accusing the Labour chair of the Work and Pensions Committee of bias and conducting a \"trial by media\".\\nSir Philip originally refused to appear before MPs last month unless Mr Field resigned as chairman.\\nHe eventually relented saying it \"would be the first and only opportunity I have had to tell my side of the very sad BHS story.\"\\n25 July 2016\\nURGENT\\nDear Sir\\nSIR PHILIP GREEN\\nWe act for Sir Philip Green and write with reference to your interview on Radio 4's Today programme this morning.\\nIn that interview you alleged that our client had stolen money, specifically from the BHS and Arcadia pension funds. This statement is highly defamatory and completely false.\\nOur client has never stolen any money from BHS, Arcadia or the pension funds and you know that. In particular, there is nothing in the recent Report of the Work and Pensions and Business, Innovation and Skills Committees, (the Report) (of which you were one of the Chairs) to support your allegation.\\nClearly an allegation that our client is a thief is likely to cause him serious harm.\\nFurther, in relation to the recent Parliamentary hearings and the Report and allegations made there you were protected by privilege. That does not apply to the interview this morning (or any others you intend to make).\\nIn the circumstances, our client requires an immediate and fulsome apology in relation to the allegation (to be agreed in terms of the content and manner by this firm in advance of publication).\\nWe look forward to hearing from you on this point within 24 hours. This matter is clearly urgent as your defamatory statements are being repeated in the media, for which you are undoubtedly liable.\\nThe other remedies to which our client is clearly entitled will very much depend on form and manner of your response and in the meantime, all of our client's rights are reserved.\\nPlease acknowledge receipt.\\nYours faithfully\\nSCHILLINGS</td>\n",
       "      <td>Former BHS owner Sir Philip Green has demanded an \"immediate apology\" from MP Frank Field for comments he made about his running of the collapsed retailer.</td>\n",
       "      <td>36889819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amateur rider Carberry, 32, has won six races at the Cheltenham Festival and ridden six times in the Grand National at Aintree.\\n\"I am going to miss the rest of the season as I am delighted to announce that I am expecting my first baby,\" she tweeted.\\nShe is married to Ted Walsh junior, the brother of jockeys Ruby and Katie.\\n\"I would like to thank all the owners and trainers who have supported me and I wish them every success in the season ahead, \" added Carberry, who won the Irish Grand National in 2011 on Organisedconfusion.</td>\n",
       "      <td>Leading female jump jockey Nina Carberry is pregnant and will miss the remainder of the season.</td>\n",
       "      <td>37944747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>He curled a brilliant 38th-minute free-kick low into the bottom corner, but injured his hamstring in the process and limped off just two minutes later.\\nMickey Demetriou should have made it 2-0 but made a mess of his effort, while Scott Vernon's shot hit the bar.\\nChris Wood headed against the post for Leicester, then nodded wide late on.\\nShrewsbury entertain Championship Norwich in the third round.\\nThe three-times League Cup winners appeared to lack urgency against the lower league opposition, whose goal only came under threat on a handful of occasions.\\nRiyad Mahrez, who was excellent for the Foxes towards the end of their promotion run last season, produced a fine save from Jayson Leutwiler, before the Swiss goalkeeper kept out Marcin Wesilewski's hooked volley.\\nShrewsbury scored the only goal seven minutes before half-time, when Mangan struck a free-kick into the bottom corner.\\nBut, even without Mangan, the League Two side also went close through James Collins, who forced a good save from Ben Hamer.\\nThe former Charlton keeper also pushed an effort from left-back Demetriou over the bar.\\nVernon's strike from 10 yards then came back off the woodwork before Demetriou scuffed his shot wide after being teed up by Collins. But the Town hung on to claim their first win on a Premier League ground.\\nFoxes manager Nigel Pearson told BBC Radio Leicester:\\n\"They created chances and good luck to them, but my focus is on our performance and I expected better.\\n\"We take cup competitions seriously and I'm annoyed with the fashion of our exit. We didn't show enough aggression and we need to strengthen.\\n\"When you get disappointing results, you've got to look at how you go about using it as a learning experience. The players should understand by now exactly what is expected, and clearly tonight it wasn't enough.\"\\nTown manager Micky Mellon told BBC Radio Shropshire:\\n\"These are the nights that are special and you've really got to savour them. When you take a League Two club to a Premier League club and knock them out, it's got to be right up there.\\n\"It wasn't just the victory. It was the way we got the victory. The way we passed, the way we defended. We were fantastic, and fully deserved it. And it was some goal to win it. A terrific strike. But, if I'm honest, I think we could have had a few more.\\n\"Whatever anyone says, however many changes Leicester made, they had guys out there who play in the Premier League, and I want to make sure my players get the pat on the back they deserve.\"</td>\n",
       "      <td>Andy Mangan's first-half effort for Shrewsbury Town proved decisive as the League Two side stunned Leicester to reach the Capital One Cup third round.</td>\n",
       "      <td>28846954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Huruma\" is a Swahili word meaning mercy.\\nIt is also the name of the district of the city that has attracted world attention following the building collapse that killed 51 people.\\nJudging by the sad events of Huruma, and many other previous tragedies, it is safe to say that the majority of Kenyans live by the mercy of God.\\nThe collapse of the building is a reflection of a society that is collapsing bit by bit, day by day.\\nThe rule of law in Kenya has all but collapsed under the weight of impunity.\\nFurthermore, many do not trust the very people whose job it is to enforce the law - the police.\\nThe Kenya National Commission on Human Rights, a watchdog body established by the constitution, has said the police have been responsible for torture, killings and summary executions.\\nWhile action has been taken against some individual officers accused of taking the law into their own hands, not all have faced justice.\\nImpunity also means you can put up an unsafe building in an unsafe area and stuff it with desperate families.\\nAnd then sleep soundly, knowing that although something really nasty could happen to your tenants, there is nothing bad that can happen to you.\\nJoseph Warungu:\\n\"The one institution in Kenya that has stood the test of time and is unlikely to collapse any day soon is corruption\"\\nFive reasons why buildings collapse\\nIn Kenya money speaks loudly, and it can silence anyone or anything.\\nMore than 30 buildings have collapsed in the last 10 years in different parts of Kenya, killing and injuring people.\\nAn audit of the safety of buildings in Nairobi, conducted last year by the National Construction Authority, showed that only 42% of the buildings were found to be safe to live in.\\nThe education system too has been teetering on the edge of collapse.\\nIn March, the national examinations board was dissolved, following widespread cheating in national exams, which resulted in the cancellation of 5,000 results.\\nSome of this cheating is inspired and funded by the parents themselves, a sign of another institution - morality - in danger of collapse.\\nMorality stands little chance once discipline in Kenya buckles.\\nIt is the indiscipline and impunity that gives people the confidence to drive dangerously in the wrong lane as police watch helplessly.\\nWe have also witnessed examples of the collapse of clear leadership in key public institutions with negative consequences.\\nDuring the 2013 terror attack on the Westgate shopping centre in Nairobi, in which 67 people were killed and 200 others injured, the rescue operation was hampered by rivalry between the police, who initially responded to the attack, and the military, who eventually took over the operation.\\nIt was a case of inflated egos, lack of co-ordination and confusion over who really was in charge.\\nSometimes it is never quite clear who is in charge in Kenya.\\nThe president himself has in the past found himself in an embarrassing position after pronouncing on one thing, or issuing a particular directive, only to be contradicted or overruled by another institution.\\nAnd this month we witnessed the collapse of yet another Kenyan virtue - hospitality.\\nThe authorities have lost patience with the hundreds of thousands of refugees, mainly from Somalia who have been sheltering in Kenya.\\nIf the government has its way on the matter, the refugees will need to find a new home either back in Somalia or in a different country.\\nBut the one institution in Kenya that has stood the test of time and is unlikely to collapse any day soon is corruption.\\nIn almost every tragedy in Kenya, money will have changed hands.\\nA study conducted in 2014 said some buildings collapse because contractors steal cement and use less steel.\\nArchitects and engineers are also blamed for failing to verify the quality of the work or properly supervise construction.\\nAnd then you have the Nairobi City building inspectors who are overstretched.\\nAs a result, the institution of corruption is always hanging about in the shadows and it never takes a day off.\\nThe police were quick to take action following the collapse of the building in Huruma and arrested the owners, but they did not target the whole chain of people involved in the construction industry.\\nThis though is Kenya where we love painkillers to deal with the symptoms instead of treating the causes of the illness.\\nAnd because of this, our lives will continue to be at the mercy of God.\\nUntil we reinforce our social, political, economic and governance structures to prevent the collapse of the Kenyan soul, the Huruma building collapse will repeat itself.\\nAnd our response will be to reach out for the predictable pill to kill the pain.\\nHopefully one day those entrusted with public positions will move the nation from huruma (mercy) to another Swahili word, huduma (service).\\nMore from Joseph Warungu:</td>\n",
       "      <td>In our series of letters from African journalists, Joseph Warungu considers what the collapse of a building in Nairobi reveals about Kenyan society.</td>\n",
       "      <td>36270281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Some are suggesting the pressure of being on 499 goals has got to the Argentine, who has scored for his country during the run, but it is not the worst spell of his career.\\nThe 28-year-old failed to find the net in his first seven games for Barca as a teenager - although the only other time he failed to score in five straight club games was in April 2010. He ended it with five in the next four.\\nBy Messi's standards, five club games without a goal is quite a goal drought. Yet some of Europe's top strikers would bite your hand off for a chance to say a five-match dry run is one of the worst spells of their careers.\\nMessi's club-mate, Brazil star Neymar, is suffering a mini-drought of his own right now. He has failed to score in the past four for Barcelona. His worst run for the Spanish giants came when he failed to score in his first six games after joining the club from Santos in 2013. He then suffered a second six-match lean spell later that same season.\\nThe third part of the feared 'MSN' trio failed to find the net in his first five games for Barcelona after leaving Liverpool in 2014. The 29-year-old Uruguay international had hit 33 in 39 games for Liverpool the previous season but that still included a run of five without a goal. He went six goals without a game when hitting 30 for Liverpool the year before that but his worse spell as an established first-teamer is nine without a goal for Ajax in 2010.\\nReal Madrid's superstar hasn't gone more than three games without a goal for five years. February 2011 was the last time he didn't register in four games. You have to go back to the 31-year-old's Manchester United days to find a longer barren spell - 11 without a goal from November 2008 to January 2009, when a strike against Premier League whipping boys Derby ended the unwanted run. The Portugal captain still managed 25 that year and switched to the Bernabeu for £80m in the summer.\\nRonaldo's former team-mate at Old Trafford has had a career punctuated by questions about his form. A graph of the Manchester United and England captain's goals record looks like a rollercoaster ride. But his five game stretch without a goal this season is nothing compared to his nine without finding the net last year. His worst, though, remains 12 games - in the 2005-06 season in which he still managed 19 goals for United - and with Everton in 2003-04.\\nRooney's last slump in form led to calls for Leicester's 21-goal striker Vardy to replace the captain in the England team. The player of the year nominee and former Fleetwood Town man has four in his last four, including two for England - but didn't find the net for five games before that. And after scoring in 11 Premier League games in a row earlier this season, the 29-year-old didn't bag for the next seven matches. Mind you, that's nothing compared to the 23 top-flight games he went without a goal as City fought relegation last season.\\nFellow player of the year nominee and England striker Kane, 22, has 28 goals so far this season. The Premier League's top scorer has never gone more than four games without bagging since breaking into the side in 2013-14.\\nBayern Munich frontman Lewandowski, 27, hasn't scored in four for the first time this season. You have to go back to 2010-11 to find the last time the Poland striker endured a longer run that that, not scoring in 10 club games during his first season with Borussia Dortmund.\\nLewandowski's successor, African footballer of the year Pierre-Emerick Aubameyang, has been on fire for Dortmund this year, with 33 goals in 39 appearances. His longest barren spell without a goal in 2015-16 is just two games. The 26-year-old Gabonese was less prolific last season and went five games without a goal in November and December. But the year before that suffered a 16-game run without scoring.\\nZlatan has 35 for French champions Paris St-Germain this season, despite taking four games to get off the mark. The 34-year-old Sweden star went five without scoring for a spell near the start of last season after a blistering seven-goals-in-four-games start at the Parc des Princes. But you need to go back to 2008 to find a worse run, of six games while at Inter Milan. A 12-game drought for Juventus a decade ago remains the worst of his career.\\nSerie A's top marksman, Argentina international Higuain, 28, has 32 goals for Napoli this season. He has only failed to score in more than four successive games for the club once since joining them from Real Madrid in 2013 - he went five without scoring in early 2014, ending the drought with a hat-trick against Lazio.</td>\n",
       "      <td>Lionel Messi has now gone five Barcelona games without netting as he chases goal number 500 for club and country in his career.</td>\n",
       "      <td>36042112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_samples(raw_datasets['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc83a72e",
   "metadata": {},
   "source": [
    "---\n",
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "848ad3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Type of Data\n",
    "type(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df2af820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the metric\n",
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9984de4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"rouge\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}, usage: \"\"\"\n",
       "Calculates average rouge scores for a list of hypotheses and references\n",
       "Args:\n",
       "    predictions: list of predictions to score. Each prediction\n",
       "        should be a string with tokens separated by spaces.\n",
       "    references: list of reference for each prediction. Each\n",
       "        reference should be a string with tokens separated by spaces.\n",
       "    rouge_types: A list of rouge types to calculate.\n",
       "        Valid names:\n",
       "        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n",
       "        `\"rougeL\"`: Longest common subsequence based scoring.\n",
       "        `\"rougeLSum\"`: rougeLsum splits text using `\"\n",
       "\"`.\n",
       "        See details in https://github.com/huggingface/datasets/issues/617\n",
       "    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n",
       "    use_aggregator: Return aggregates if this is set to True\n",
       "Returns:\n",
       "    rouge1: rouge_1 (precision, recall, f1),\n",
       "    rouge2: rouge_2 (precision, recall, f1),\n",
       "    rougeL: rouge_l (precision, recall, f1),\n",
       "    rougeLsum: rouge_lsum (precision, recall, f1)\n",
       "Examples:\n",
       "\n",
       "    >>> rouge = datasets.load_metric('rouge')\n",
       "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
       "    >>> references = [\"hello there\", \"general kenobi\"]\n",
       "    >>> results = rouge.compute(predictions=predictions, references=references)\n",
       "    >>> print(list(results.keys()))\n",
       "    ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
       "    >>> print(results[\"rouge1\"])\n",
       "    AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))\n",
       "    >>> print(results[\"rouge1\"].mid.fmeasure)\n",
       "    1.0\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7e80793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_preds = [\"hello there\", \"general kenobi\"]\n",
    "fake_labels = [\"hello there\", \"general kenobi\"]\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9994aad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model checkpoint of pretrained model \n",
    "model_checkpoint = \"t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecbe91ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37494fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [8774, 6, 48, 19, 3, 9, 7142, 55, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Tokenizer on different sentences for checking\n",
    "tokenizer(\"Hello, this is a sentence!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b466d449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[8774, 6, 48, 19, 3, 9, 7142, 55, 1], [100, 19, 430, 7142, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"Hello, this is a sentence!\", \"This is another sentence.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b486944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[8774, 6, 48, 19, 3, 9, 7142, 55, 1], [100, 19, 430, 7142, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    print(tokenizer([\"Hello, this is a sentence!\", \"This is another sentence.\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da140da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using one of the five T5 checkpoints we have to prefix the inputs with \"summarize:\" \n",
    "#(the model can also translate and it needs the prefix to know which task it has to perform).\n",
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"summarize: \"\n",
    "else:\n",
    "    prefix = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4873d9ab",
   "metadata": {},
   "source": [
    "We have written the function that will preprocess our samples. We just feed them to the tokenizer with the argument truncation=True. This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model. The padding will be dealt with later on (in a data collator) so we pad examples to the longest length in the batch and not the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fca117ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"summary\"], max_length=max_target_length, truncation=True\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c04be9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"summary\"], max_length=max_target_length, truncation=True\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "513ac469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function works with one or several examples. In the case of several examples,\n",
    "# the tokenizer will return a list of lists for each key:\n",
    "# preprocess_function(raw_datasets[\"train\"][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868b2e79",
   "metadata": {},
   "source": [
    "To apply this function on all the pairs of sentences in our dataset, we just use the map method of our dataset object we created earlier. This will apply the function on all the elements of all the splits in dataset, so our training, validation and testing data will be preprocessed in one single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f19e1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/jvdboss/workspace/Testing_Stuff/summarization_tensorflow/final_data/train/cache-91d5b0c8d787a43b.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/jvdboss/workspace/Testing_Stuff/summarization_tensorflow/final_data/validation/cache-88923bf6342ea0ed.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/jvdboss/workspace/Testing_Stuff/summarization_tensorflow/final_data/test/cache-e706f6542e2ada3f.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0cf0438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e516bebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/jvdboss/workspace/Testing_Stuff/summarization_tensorflow/final_data/train/cache-0689c88f462d5410.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/jvdboss/workspace/Testing_Stuff/summarization_tensorflow/final_data/validation/cache-876281ac29164311.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/jvdboss/workspace/Testing_Stuff/summarization_tensorflow/final_data/test/cache-036ab76e8634c258.arrow\n"
     ]
    }
   ],
   "source": [
    "# let's take the sample of train, validation and test dataset \n",
    "# If you have enough memory and resources you can use the entire data for the project.\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(6000))\n",
    "small_validation_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(1000))\n",
    "small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34a7054",
   "metadata": {},
   "source": [
    "---\n",
    "Now that our data is ready, we can download the pretrained model and fine-tune it. Since our task is sequence-to-sequence (both the input and output are text sequences), we use the AutoModelForSeq2SeqLM class. Like with the tokenizer, the from_pretrained method will download and cache the model for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53f227cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-14 18:03:13.584861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 18:03:13.730848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 18:03:13.731013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 18:03:13.731923: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-14 18:03:13.733125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 18:03:13.733356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 18:03:13.733541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 18:03:14.364393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 18:03:14.364577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 18:03:14.364712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 18:03:14.364824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2592 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed2aabe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define the cosntants \n",
    "batch_size = 1\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "num_train_epochs = 1\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "push_to_hub_model_id = f\"{model_name}-finetuned-xsum\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fb9e60",
   "metadata": {},
   "source": [
    "Then, we need a special kind of data collator, which will not only pad the inputs to the maximum length in the batch, but also the labels. Note that our data collators are multi-framework, so make sure you set return_tensors='tf' so you get tf.Tensor objects back and not something else!\n",
    "\n",
    "We also want to compute ROUGE metrics, which will require us to generate text from our model. To speed things up, we can compile our generation loop with XLA. This results in a huge speedup - up to 100X! The downside of XLA generation, though, is that it doesn't like variable input shapes, because it needs to run a new compilation for each new input shape! To compensate for that, let's use pad_to_multiple_of for the dataset we use for text generation. This will reduce the number of unique input shapes a lot, meaning we can get the benefits of XLA generation with only a few compilations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "723773f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")\n",
    "\n",
    "generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\", pad_to_multiple_of=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5338aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'summary', 'id', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 204045\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2bc91b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'summary', 'id', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 6000\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a539bd0",
   "metadata": {},
   "source": [
    "Next, we convert our datasets to tf.data.Dataset, which Keras understands natively. There are two ways to do this - we can use the slightly more low-level Dataset.to_tf_dataset() method, or we can use Model.prepare_tf_dataset(). The main difference between these two is that the Model method can inspect the model to determine which column names it can use as input, which means you don't need to specify them yourself. Make sure to specify the collator we just created as our collate_fn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6892779f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = model.prepare_tf_dataset(\n",
    "    small_train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "validation_dataset = model.prepare_tf_dataset(\n",
    "    small_validation_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "test_dataset = model.prepare_tf_dataset(\n",
    "    small_test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bce5c21",
   "metadata": {},
   "source": [
    "Now we initialize our loss and optimizer and compile the model. Note that most Transformers models compute loss internally - we can train on this as our loss value simply by not specifying a loss when we compile()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81318c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b228b3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_fn(eval_predictions):\n",
    "    predictions, labels = eval_predictions\n",
    "    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    for label in labels:\n",
    "        label[label < 0] = tokenizer.pad_token_id  # Replace masked label tokens\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_predictions = [\n",
    "        \"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_predictions\n",
    "    ]\n",
    "    decoded_labels = [\n",
    "        \"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels\n",
    "    ]\n",
    "    result = metric.compute(\n",
    "        predictions=decoded_predictions, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    # Add mean generated length\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eb6eb2",
   "metadata": {},
   "source": [
    "---\n",
    "### Train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "839ee652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 787s 129ms/step - loss: 2.9422 - val_loss: 2.6128\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f2e31e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [2.942176342010498], 'val_loss': [2.612797498703003]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e56c09",
   "metadata": {},
   "source": [
    "---\n",
    "### Evaluate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95b67f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 38s 38ms/step - loss: 2.5948\n",
      "2.594815731048584\n"
     ]
    }
   ],
   "source": [
    "# Let's check the loss of the model \n",
    "loss = model.evaluate(test_dataset)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f224a3",
   "metadata": {},
   "source": [
    "---\n",
    "### Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d00f8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the pretrained model\n",
    "model.save_pretrained(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c643210b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5e05a9",
   "metadata": {},
   "source": [
    "---\n",
    "### Taking custom input to check the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b34c7fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (539 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "document = 'The full cost of damage in Newton Stewart, one of the areas worst affected, is still being assessed.\\nRepair work is ongoing in Hawick and many roads in Peeblesshire remain badly affected by standing water.\\nTrains on the west coast mainline face disruption due to damage at the Lamington Viaduct.\\nMany businesses and householders were affected by flooding in Newton Stewart after the River Cree overflowed into the town.\\nFirst Minister Nicola Sturgeon visited the area to inspect the damage.\\nThe waters breached a retaining wall, flooding many commercial properties on Victoria Street - the main shopping thoroughfare.\\nJeanette Tate, who owns the Cinnamon Cafe which was badly affected, said she could not fault the multi-agency response once the flood hit.\\nHowever, she said more preventative work could have been carried out to ensure the retaining wall did not fail.\\n\"It is difficult but I do think there is so much publicity for Dumfries and the Nith - and I totally appreciate that - but it is almost like we\\'re neglected or forgotten,\" she said.\\n\"That may not be true but it is perhaps my perspective over the last few days.\\n\"Why were you not ready to help us a bit more when the warning and the alarm alerts had gone out?\"\\nMeanwhile, a flood alert remains in place across the Borders because of the constant rain.\\nPeebles was badly hit by problems, sparking calls to introduce more defences in the area.\\nScottish Borders Council has put a list on its website of the roads worst affected and drivers have been urged not to ignore closure signs.\\nThe Labour Party\\'s deputy Scottish leader Alex Rowley was in Hawick on Monday to see the situation first hand.\\nHe said it was important to get the flood protection plan right but backed calls to speed up the process.\\n\"I was quite taken aback by the amount of damage that has been done,\" he said.\\n\"Obviously it is heart-breaking for people who have been forced out of their homes and the impact on businesses.\"\\nHe said it was important that \"immediate steps\" were taken to protect the areas most vulnerable and a clear timetable put in place for flood prevention plans.\\nHave you been affected by flooding in Dumfries and Galloway or the Borders? Tell us about your experience of the situation and how it was handled. Email us on selkirk.news@bbc.co.uk or dumfries@bbc.co.uk.'\n",
    "if 't5' in model_name: \n",
    "    document = \"summarize: \" + document\n",
    "tokenized = tokenizer([document], return_tensors='np')\n",
    "out = model.generate(**tokenized, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5519d766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> The damage to the roads in the Scottish Borders has been a \"wonderful and aback\" for people who have been forced out of their homes, the Labour Party has said.</s>\n"
     ]
    }
   ],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    print(tokenizer.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcf1885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a281bf39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96c8ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d927da31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "611a46306b8dbc2cae382b4d5e540b84c7a44844fb2c83e5ce0798aa9b4b90fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
